---
title: "My Project Report"
format: gfm
---

## Milestone 1: Project Idea

I love anything airline related, so my idea is to analyze some airline data to see if one thing causes another. Specifically, I want to see if having an airline's loyalty card increases the revenue that customer makes for the airline (called CLV or Customer Lifetime Value).

## Milestone 2: Data Story

Causal Inference Question:
Does having a higher loyalty card status (Star, Nova, Aurora) cause an increase in CLV?

Context:
I want to study if having a higher loyalty card status for an airline causes an increase in CLV. CLV is Customer Lifetime Value. It is computed as the sum of all revenues (or invoices) generated by the customer for their flight bookings over their entire membership period. Basically, it's how much total revenue a specific customer is expected to generate. The Star loyalty card is the highest, Nova is the next highest, and Aurora is third highest. In the data, only these 3 loyalties are listed.

The outcome would be positive and continuous values because it's how much revenue is expected to be made under the CLV. Other predictor variables that could influence CLV is salary, education level, marital status, total flights, and enrollment year/month. Other variables that could influence our outcome could be country of residence, gender, distance traveled, points redeemed, and dollar cost of points redeemed.

My story is that I believe that having a loyalty card increases the revenue you will make for the airline.

https://www.kaggle.com/datasets/agungpambudi/airline-loyalty-campaign-program-impact-on-flights/data


## Milestone 3: DAG

This is the first DAG I created.

![Initial Old DAG](../figures/Old_DAG.png)

This is the second DAG I created. I add more variables because I have most of these columns in my dataset.

![Second Iteration of Initial Old DAG](../figures/Old_DAG_Updated.png)

## Milestone 4: Identification Strategy

Based on the DAG, here are all possible paths (23 paths) from LCS (Loyalty Card Status) to CLV (Customer Lifetime Value):

* LCS, CLV
* LCS, I, CLV
* LCS, I, TF, CLV
* LCS, I, TF, FD, CLV
* LCS, I, TF, CMS, CLV
* LCS, I, TF, CMS, CE, CLV
* LCS, TF, CLV
* LCS, TF, I, CLV
* LCS, TF, FD, CLV
* LSC, TF, CMS, CLV
* LCS, TF, CMS, CE, CLV
* LCS, CMS, CLV
* LCS, CMS, CE, CLV
* LCS, CMS, TF, CLV
* LCS, CMS, TF, FD, CLV
* LCS, CMS, TF, I, CLV
* LCS, CE, CLV
* LCS, CE, CMS, CLV
* LCS, CE, CMS, TF, CLV
* LCS, CE, CMS, TF, FD, CLV
* LCS, CE, CMS, TF, I, CLV
* LCS, UP, CLV
* LCS, LTE, CLV

Direct pipes, Can estimate total causal effect through them

* LCS, UP, CLV
* LCS, LTE, CLV

Here are the backdoors & what to do about them:

* LCS, I, CLV  --  Fork, Condition on I
* LCS, I, TF, CLV -- Fork, Pipe, Condition on TF
* LCS, I, TF, FD, CLV -- Fork, Pipe, Pipe, Condition on FD (?)
* LCS, I, TF, CMS, CLV --Fork, Collider, Fork, Condition on CMS
* LCS, I, TF, CMS, CE, CLV -- Fork, Collider, Fork, Pipe, Condition on CE (??)
* LCS, TF, CLV -- Fork, Condition on TF
* LCS, TF, I, CLV -- Fork, Pipe, Condition on I
* LCS, TF, FD, CLV -- Fork, Pipe, Condition on FD (?)
* LSC, TF, CMS, CLV -- Pipe, Fork, Condition on CMS
* LCS, TF, CMS, CE, CLV -- Pipe, Fork, Pipe, Condition on CE
* LCS, CMS, CLV -- Fork, Condition on CMS
* LCS, CMS, CE, CLV -- Fork, Pipe, Condition on CE (?)
* LCS, CMS, TF, CLV -- Fork, Pipe, Condition on CMS (?)
* LCS, CMS, TF, FD, CLV -- Fork, Pipe, Pipe, Condition on CMS & TF (??)
* LCS, CMS, TF, I, CLV -- Fork, Collider, Fork, Condition on CMS, TF, & I (all are needed in the end I believe) (?) 
* LCS, CE, CLV -- Fork, Condition on CE
* LCS, CE, CMS, CLV -- Pipe, Fork, Condition on CE or CMS
* LCS, CE, CMS, TF, CLV -- Pipe, Fork, Pipe, Condition on CE or CMS or TF (all are needed in the end I believe) (?)
* LCS, CE, CMS, TF, FD, CLV -- Pipe, Fork, Pipe, Pipe, Condition on CE or CMS or TF (all are needed in the end I believe) (?)
* LCS, CE, CMS, TF, I, CLV -- Pipe, Fork, Collider, Fork, Condition on CE or CMS or TF or I (all are needed in the end I believe) (?)

Adjustment Set: I, TF, CMS, and CE.

## Milestone 5: Simulate Data and Recover Parameters

```{python}
import numpy as np
import polars as pl
import seaborn as sns
from sklearn.linear_model import LinearRegression

np.random.seed(42)

# Set the parameter values.
beta0 = 1000
income = 70000
flight_dist = 3500
travel_freq = 5
cust_marketing_strat = 1
cust_engagement = 3
loyalty_card_status = 1000
n = 100

sim_data = (
    # Simulate predictors using appropriate np.random distributions.
    pl.DataFrame({
        'x': np.random.uniform(0, 7, size=n),
        'flight_dist': np.random.uniform(0, 10000, size=n),
        'travel_freq': np.random.uniform(1, 10, size=n),
        'cust_marketing_strat': np.random.uniform(0, 5, size=n),
        'cust_engagement': np.random.uniform(1, 10, size=n),
        'loyalty_card_status': np.random.choice([0, 1], size=n)  # New binary predictor
    })
    # Use predictors and parameter values to simulate the outcome.
    .with_columns([ 
        (
            beta0 + income * pl.col('x') + flight_dist * pl.col('flight_dist') +
            travel_freq * pl.col('travel_freq') +
            cust_marketing_strat * pl.col('cust_marketing_strat') +
            cust_engagement * pl.col('cust_engagement') +
            loyalty_card_status * pl.col('loyalty_card_status') +  # Adding loyalty_card_status with a coefficient (e.g., 1000)
            np.random.normal(0, 3, size=n)
        ).alias('CLV')  # Renaming y to CLV
    ])
)

# Display the data
sim_data

# Visualize the data
# sns.scatterplot(data=sim_data, x='x', y='CLV')
# sns.lmplot(data=sim_data, x='x', y='CLV', height=6, aspect=1, scatter_kws={'s': 10}, line_kws={'color': 'red'})

# Specify the X matrix and CLV vector.
X = sim_data[['x', 'flight_dist', 'travel_freq', 'cust_marketing_strat', 'cust_engagement', 'loyalty_card_status']]
CLV = sim_data['CLV']

# Create a linear regression model.
model = LinearRegression(fit_intercept=True)
# Train the model.
model.fit(X, CLV)

# Print the coefficients
print(f'Intercept: {model.intercept_}')
print(f'Slope for x: {model.coef_[0]}')
print(f'Slope for flight_dist: {model.coef_[1]}')
print(f'Slope for travel_freq: {model.coef_[2]}')
print(f'Slope for cust_marketing_strat: {model.coef_[3]}')
print(f'Slope for cust_engagement: {model.coef_[4]}')
print(f'Slope for loyalty_card_status: {model.coef_[5]}')

# Have you recovered the parameters?
# Yes
```

In this code, a simulated dataset is created using various predictors (independent variables) to estimate the Customer Lifetime Value (CLV), which is the outcome (dependent variable). The code first defines the parameter values for each predictor, including factors like income, flight distance, travel frequency, customer marketing strategy, customer engagement, and loyalty card status. It then generates random values for these predictors and combines them in a linear equation to simulate CLV. This equation also includes a small amount of random noise to make the data more realistic.

After generating the data, a linear regression model is applied to it using the `LinearRegression` function from the `sklearn` library. The model is trained using the predictors (`X`) and the simulated CLV values. Once the model is trained, the code prints the estimated coefficients for each predictor, which represent how each variable impacts the CLV. By examining these coefficients, we can understand the relationship between the predictors and CLV.

## Milestone 6: Exploratory Data Analysis

Aliquam sociosqu habitant conubia porta sagittis sociosqu aenean? Nunc eget accumsan nunc lacus, urna lacus? Est quisque quis iaculis phasellus nisl. Purus nisi cursus convallis, tristique mauris sagittis nibh.

```{python}
#| eval: false
(so.Plot(sim_data, x = 'x', y = 'y')
  .add(so.Dot(pointsize = 10, alpha = 0.5))
)
```

![](../figures/sim-data-01.png){fig-align="center"}

Mauris volutpat iaculis enim nam taciti est ipsum dui.

## Milestone 7: Estimate Causal Effects

```{python}
# eval: false
import bambi as bmb
import arviz as az

# Import foxes data.
foxes = pl.read_csv('../data/foxes.csv')

# Use Bambi to estimate the direct causal effect of avgfood on weight.
bambi_model_01 = bmb.Model('weight ~ avgfood + groupsize', foxes.to_pandas())
bambi_model_01
```

```
       Formula: weight ~ avgfood + groupsize
        Family: gaussian
          Link: mu = identity
  Observations: 116
        Priors: 
    target = mu
        Common-level effects
            Intercept ~ Normal(mu: -0.0, sigma: 2.4892)
            avgfood ~ Normal(mu: 0.0, sigma: 2.5)
            groupsize ~ Normal(mu: 0.0, sigma: 2.5)
        
        Auxiliary parameters
            sigma ~ HalfStudentT(nu: 4.0, sigma: 0.9957)
```

```{python}
#| eval: false
# Calls pm.sample().
bambi_fit_01 = bambi_model_01.fit()
az.plot_trace(bambi_fit_01, compact = False)
```

![](../figures/multilevel-models_plot-01.png){fig-align="center"}

```{python}
#| eval: false
# Visualize marginal posteriors.
az.plot_forest(bambi_fit_01, var_names = ['avgfood', 'groupsize'], combined = True, hdi_prob = 0.95)
```

![](../figures/multilevel-models_plot-02.png){fig-align="center"}

Controlling for group size, average food has a positive impact on fox weight.

## Milestone 8: Intermediate Presentation

See my intermediate presentation [slides](https://github.com/marcdotson/causal-inference/blob/main/presentations/multivariate-models.html). To summarize some feedback:

- Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
- Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
- Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.
- Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
